# -*- coding: utf-8 -*-
"""FLcos-CIFAR-10-alexnet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dQBUF_RZNtCPwtoypbEjGeEdwMYqClS0
"""

# import os,sys
# os.chdir("/content/drive/MyDrive/posingattack/NDSS21-Model-Poisoning/cifar10")
# sys.path.insert(0, '/content/drive/MyDrive/posingattack/NDSS21-Model-Poisoning/utils')

# import lib
import argparse, os, sys, csv, shutil, time, random, operator, pickle, ast, math
import numpy as np
import pandas as pd
from torch.optim import Optimizer
import torch.nn.functional as F
import torch
import pickle
import torch.nn as nn
import torch.nn.parallel
import torch.backends.cudnn as cudnn
import torch.optim as optim
import torch.utils.data as data
import torch.multiprocessing as mp
sys.path.insert(0,'./../utils/')
# sys.path.insert(0, '/content/drive/MyDrive/posingattack/NDSS21-Model-Poisoning/utils')

from logger import *
from eval import *
from misc import *

from cifar10_normal_train import *
from cifar10_util import *
from adam import Adam
from sgd import SGD

import torchvision.transforms as transforms
import torchvision.datasets as datasets

#set seed
def set_seed(seed_value=42):
    torch.manual_seed(seed_value)
    torch.cuda.manual_seed(seed_value)
    torch.cuda.manual_seed_all(seed_value)
    np.random.seed(seed_value)
    random.seed(seed_value)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# set_seed(42)
def reset_seed(seed=42):
    """重置随机种子，确保实验可复现。"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)  # 如果使用多GPU
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def reset_torch_state():
    """重置PyTorch的全局状态，减少上一个实验对下一个实验的潜在干扰。"""
    torch.cuda.empty_cache()  # 清空CUDA缓存
    # 根据需要重置其它全局状态
#import dataset CIFAR10
#code from link: https://github.com/Jenson66/Poisoning-Attack-on-FL/blob/main/models/main.py
#and https://github.com/vrt1shjwlkr/NDSS21-Model-Poisoning/blob/main/cifar10/release-fedsgd-alexnet-bulyan-unknown-benign-gradients.ipynb


#AGR-agnostic
def Shejwalkar_attack_mkrum(all_updates, model_re, n_attackers, dev_type='unit_vec'):

    if dev_type == 'unit_vec':
        deviation = model_re / torch.norm(model_re)
    elif dev_type == 'sign':
        deviation = torch.sign(model_re)
    elif dev_type == 'std':
        deviation = torch.std(all_updates, 0)

    lamda = torch.Tensor([3.0]).cuda()

    threshold_diff = 1e-5
    lamda_fail = lamda
    lamda_succ = 0

    while torch.abs(lamda_succ - lamda) > threshold_diff:
        mal_update = (model_re - lamda * deviation)
        mal_updates = torch.stack([mal_update] * n_attackers)
        mal_updates = torch.cat((mal_updates, all_updates), 0)

        agg_grads, krum_candidate = Multi_krum(mal_updates, n_attackers, multi_k=True)
        if np.sum(krum_candidate < n_attackers) == n_attackers:
            # print('successful lamda is ', lamda)
            lamda_succ = lamda
            lamda = lamda + lamda_fail / 2
        else:
            lamda = lamda - lamda_fail / 2

        lamda_fail = lamda_fail / 2

    mal_update = (model_re - lamda_succ * deviation)
    return mal_update

def compute_lambda_fang(all_updates, model_re, n_attackers):

    distances = []
    n_benign, d = all_updates.shape
    for update in all_updates:
        distance = torch.norm((all_updates - update), dim=1)
        distances = distance[None, :] if not len(distances) else torch.cat((distances, distance[None, :]), 0)

    distances[distances == 0] = 10000
    distances = torch.sort(distances, dim=1)[0]
    scores = torch.sum(distances[:, :n_benign - 2 - n_attackers], dim=1)
    min_score = torch.min(scores)
    term_1 = min_score / ((n_benign - n_attackers - 1) * torch.sqrt(torch.Tensor([d]))[0])
    max_wre_dist = torch.max(torch.norm((all_updates - model_re), dim=1)) / (torch.sqrt(torch.Tensor([d]))[0])

    return (term_1 + max_wre_dist)


def fang_attack_krum_partial(all_updates, model_re, deviation, n_attackers):

    lamda = compute_lambda_fang(all_updates, model_re, n_attackers)
    threshold = 1e-5

    mal_updates = []
    while lamda > threshold:
        mal_update = (- lamda * deviation)

        mal_updates = torch.stack([mal_update] * n_attackers)
        mal_updates = torch.cat((mal_updates, all_updates), 0)
        # print(mal_updates.shape)

        agg_grads, krum_candidate = krum(mal_updates, n_attackers)

        if (krum_candidate < n_attackers).any():
            return mal_update

        lamda *= 0.5

    if not len(mal_updates):
        print(lamda, threshold)
        mal_update = (model_re - lamda * deviation)

    return mal_update
def fang_attack_bulyan_partial(all_updates, model_re, deviation, n_attackers):

    lamda = compute_lambda_fang(all_updates, model_re, n_attackers)
    threshold = 1e-5

    mal_updates = []
    while lamda > threshold:
        mal_update = (- lamda * deviation)

        mal_updates = torch.stack([mal_update] * n_attackers)
        mal_updates = torch.cat((mal_updates, all_updates), 0)

        agg_grads, krum_candidate = Bulyan(mal_updates, n_attackers)

        if (krum_candidate < n_attackers).any():
            return mal_update

        lamda *= 0.5

    if not len(mal_updates):
        print(lamda, threshold)
        mal_update = (model_re - lamda * deviation)

    return mal_update
def fang_attack_trmean_partial(all_updates, n_attackers):

    model_re = torch.mean(all_updates, 0)
    model_std = torch.std(all_updates, 0)
    deviation = torch.sign(model_re)

    max_vector_low = model_re + 3 * model_std
    max_vector_hig = model_re + 4 * model_std
    min_vector_low = model_re - 4 * model_std
    min_vector_hig = model_re - 3 * model_std

    max_range = torch.cat((max_vector_low[:,None], max_vector_hig[:,None]), dim=1)
    min_range = torch.cat((min_vector_low[:,None], min_vector_hig[:,None]), dim=1)

    rand = torch.from_numpy(np.random.uniform(0, 1, [len(deviation), n_attackers])).type(torch.FloatTensor).cuda()

    max_rand = torch.stack([max_range[:, 0]] * rand.shape[1]).T + rand * torch.stack([max_range[:, 1] - max_range[:, 0]] * rand.shape[1]).T
    min_rand = torch.stack([min_range[:, 0]] * rand.shape[1]).T + rand * torch.stack([min_range[:, 1] - min_range[:, 0]] * rand.shape[1]).T

    mal_vec = (torch.stack([(deviation > 0).type(torch.FloatTensor)] * max_rand.shape[1]).T.cuda() * max_rand + torch.stack(
        [(deviation > 0).type(torch.FloatTensor)] * min_rand.shape[1]).T.cuda() * min_rand).T
    # print(mal_vec.view(-1).shape)
    return mal_vec

# AGR-agnostic
def lie_attack(all_updates, z):
    avg = torch.mean(all_updates, dim=0)
    std = torch.std(all_updates, dim=0)
    return avg + z * std
'''
MIN-MAX attack
'''
def Shejwalkar_attack_min_max(all_updates, model_re, n_attackers, dev_type='unit_vec'):

    if dev_type == 'unit_vec':
        deviation = model_re / torch.norm(model_re)  # unit vector, dir opp to good dir
    elif dev_type == 'sign':
        deviation = torch.sign(model_re)
    elif dev_type == 'std':
        deviation = torch.std(all_updates, 0)

    lamda = torch.Tensor([10.0]).float().cuda()
    # print(lamda)
    threshold_diff = 1e-5
    lamda_fail = lamda
    lamda_succ = 0

    distances = []
    for update in all_updates:
        distance = torch.norm((all_updates - update), dim=1) ** 2
        distances = distance[None, :] if not len(distances) else torch.cat((distances, distance[None, :]), 0)

    max_distance = torch.max(distances)
    del distances

    while torch.abs(lamda_succ - lamda) > threshold_diff:
        mal_update = (model_re - lamda * deviation)
        distance = torch.norm((all_updates - mal_update), dim=1) ** 2
        max_d = torch.max(distance)

        if max_d <= max_distance:
            # print('successful lamda is ', lamda)
            lamda_succ = lamda
            lamda = lamda + lamda_fail / 2
        else:
            lamda = lamda - lamda_fail / 2

        lamda_fail = lamda_fail / 2

    mal_update = (model_re - lamda_succ * deviation)

    return mal_update
'''
MIN-SUM attack
'''

def Shejwalkar_attack_min_sum(all_updates, model_re, n_attackers, dev_type='unit_vec'):

    if dev_type == 'unit_vec':
        deviation = model_re / torch.norm(model_re)  # unit vector, dir opp to good dir
    elif dev_type == 'sign':
        deviation = torch.sign(model_re)
    elif dev_type == 'std':
        deviation = torch.std(all_updates, 0)

    lamda = torch.Tensor([10.0]).float().cuda()
    # print(lamda)
    threshold_diff = 1e-5
    lamda_fail = lamda
    lamda_succ = 0

    distances = []
    for update in all_updates:
        distance = torch.norm((all_updates - update), dim=1) ** 2
        distances = distance[None, :] if not len(distances) else torch.cat((distances, distance[None, :]), 0)

    scores = torch.sum(distances, dim=1)
    min_score = torch.min(scores)
    del distances

    while torch.abs(lamda_succ - lamda) > threshold_diff:
        mal_update = (model_re - lamda * deviation)
        distance = torch.norm((all_updates - mal_update), dim=1) ** 2
        score = torch.sum(distance)

        if score <= min_score:
            # print('successful lamda is ', lamda)
            lamda_succ = lamda
            lamda = lamda + lamda_fail / 2
        else:
            lamda = lamda - lamda_fail / 2

        lamda_fail = lamda_fail / 2

    # print(lamda_succ)
    mal_update = (model_re - lamda_succ * deviation)

    return mal_update


def calculate_cos_angle(a, b):
    a_flat = a.view(-1)
    b_flat = b.view(-1)
    cos_angle = torch.dot(a_flat, b_flat) / (torch.norm(a_flat) * torch.norm(b_flat))
    return cos_angle

global previous_error
previous_error = 0  # 上一次的误差
global save_update
save_update =None
def adjust_lamda_for_target_cos_angle(all_updates, model_re, n_attackers, dev_type='unit_vec'):
    if dev_type == 'unit_vec':
        deviation = model_re / torch.norm(model_re)  # unit vector, direction opposite to good direction
    elif dev_type == 'sign':
        deviation = torch.sign(model_re)
    elif dev_type == 'std':
        deviation = torch.std(all_updates, 0)

    lamda = torch.Tensor([10.0]).float().cuda()
    threshold_diff = 1e-5
    lamda_fail = lamda
    lamda_succ = 0
    
    angles = []
    mean_vector = torch.mean(all_updates, dim=0)

    grad = []
    grad_magnitude_mean = torch.sqrt(torch.sum(mean_vector**2))
    average_direction = mean_vector / torch.norm(mean_vector)
    for update in all_updates:
        angle = calculate_cos_angle(update, average_direction)
        angles.append(angle)
        grad_magnitude = torch.sqrt(torch.sum(update**2))
        grad.append(grad_magnitude)
    global save_update
    if sum(grad_magnitude_mean > g for g in grad) >= len(grad) / 2:  # 原始向量指向彼此接近的方向
        if save_update is not None:
            # 这时候是两轮记忆了，但是现阶段的还是不行，说明还在寻找最低点，很迷茫
            prev_mean_vector = torch.mean(save_update, dim=0)
            prev_grad_magnitude_mean = torch.sqrt(torch.sum(prev_mean_vector ** 2))
            new_mean_vector = (max(grad) - prev_grad_magnitude_mean) * prev_mean_vector + (
                        min(grad) - grad_magnitude_mean) * mean_vector
            target_angle = (calculate_cos_angle(average_direction, new_mean_vector))

            # target_angle=
        else:
            # 这时候是一轮记忆，选择梯度下降最慢的
            new_mean_vector = mean_vector - (max(grad) - grad_magnitude_mean) * mean_vector
            target_angle = abs((calculate_cos_angle(average_direction, new_mean_vector)))

    else:  # 原始向量方向相反或分散
        if save_update is not None:
            # 这时候是两轮记忆了，但是现阶段的还是不行，说明还在寻找最低点，很迷茫
            prev_mean_vector = torch.mean(save_update, dim=0)
            prev_grad_magnitude_mean = torch.sqrt(torch.sum(prev_mean_vector ** 2))
            new_mean_vector = (max(grad) - prev_grad_magnitude_mean) * prev_mean_vector + (
                        grad_magnitude_mean-prev_grad_magnitude_mean) * mean_vector
            target_angle = ((calculate_cos_angle(new_mean_vector, average_direction)))
            # target_angle=angles[grad.index(max(grad))]-angle

        else:
            # 这时候是一轮记忆，选择梯度下降最慢的
            target_angle = angles[grad.index(max(grad))]

    #if (sorted_angles[0]-sorted_angles[-1])>0.5:
     #   target_angle = sorted_angles[4]
    #elif (sorted_angles[0]-sorted_angles[-1])<=0.15 and (sorted_angles[0]-sorted_angles[-1])>=0.05:
      #  target_angle = sorted_angles[-1]-0.1
    #elif (sorted_angles[0]-sorted_angles[-1])<0.05:
       # target_angle = sorted_angles[1]-0.2
    #else:
     #   target_angle = sorted_angles[1]
    #chazhi = sorted_angles[0]-sorted_angles[-1]
    #target_angle  = chazhi*chazhi/2 -0.4 * chazhi +sorted_angles[1] -(-chazhi+0.4)*(chazhi)/3
    def calculate_really_target_angle_from_sorted_angles(sorted_angles):
    # Calculate the required features from the sorted angles list
        max_angle = max(sorted_angles)
        median_angle = sorted(sorted_angles)[len(sorted_angles) // 2]
        mean_angle = sum(sorted_angles) / len(sorted_angles)
    
    # Calculate "Really Target Angle" using the previously defined formula
        coeff_max_angle = 0.777
        coeff_median_angle = 0.027
        coeff_mean_angle = -0.036#041
        intercept = 0.202
    
        really_target_angle = (coeff_max_angle * max_angle) + \
                          (coeff_median_angle * median_angle) + \
                          (coeff_mean_angle * mean_angle) + \
                          intercept
        print("-------------------------")
        print("y=0.777*",max_angle," + 0.027*",median_angle,"+ -0.036*",mean_angle,"+0.202")
        return really_target_angle
    #target_angle_fun = lambda x: 13.54 - 50.76 * x + 65.31 * (x**2) - 27.32 * (x**3)
    #target_angle = target_angle_fun(sorted_angles[0])
    #target_angle = calculate_really_target_angle_from_sorted_angles(sorted_angles)
    #print('sorted_angles:',sorted_angles,'target_angle',target_angle)
    # if sum(grad_magnitude_mean > g for g in grad)>= len(grad) / 2:
    #     target_angle =1-angles[grad.index(min(grad))]
    # else:
    #     target_angle=angles[grad.index(max(grad))]

    while torch.abs(lamda_succ - lamda) > threshold_diff:
        mal_update = model_re - lamda * deviation
        current_angle = calculate_cos_angle(mal_update, average_direction)
        if current_angle > (target_angle):
            lamda_succ = lamda
            lamda = lamda + lamda_fail / 2
        else:
            lamda = lamda - lamda_fail / 2

        lamda_fail = lamda_fail / 2

    mal_update = model_re - lamda_succ * deviation
    save_update = all_updates
    print('lamda_succ:',lamda_succ,'target_angle:',target_angle)
    return mal_update

def calculate_angle_based_weights(theta_matrix,modified_grads):
    num_tasks = theta_matrix.size(0)
    w = torch.zeros(num_tasks, device='cuda')
    normalization_factor = 0
    for i in range(num_tasks):
        # Calculate cosine similarity between original and modified gradients
        cosine_similarity = torch.dot(theta_matrix[i].flatten(), modified_grads[i].flatten()) / (
                torch.norm(theta_matrix[i]) * torch.norm(modified_grads[i])
        )
        angle_difference = cosine_similarity  # Larger differences get lower similarity, hence higher weights
        w[i] += angle_difference
        normalization_factor += angle_difference
    w /= normalization_factor
    return w
global pred_mal_update
pred_mal_update = None
def our_attack_proj(all_updates, model_re, n_attackers, dev_type='unit_vec'):
    mean_vector = torch.mean(all_updates, dim=1, keepdim=True)
    # 中心化梯度矩阵
    centered_matrix = all_updates - mean_vector
    # 计算协方差矩阵
    covariance_matrix = centered_matrix @ centered_matrix.t() / (all_updates.size(1) - 1)
    del centered_matrix
    # 计算标准差
    std_dev = torch.sqrt(torch.diag(covariance_matrix))
    # 从协方差矩阵计算相关矩阵
    correlation_matrix = covariance_matrix / torch.outer(std_dev, std_dev)
    del covariance_matrix, std_dev
    # 计算特征值和特征向量
    eigenvalues, eigenvectors = torch.linalg.eigh(correlation_matrix)
    del correlation_matrix
    # 找到最大特征值对应的特征向量
    max_eigenvalue, idx = torch.max(eigenvalues, dim=0)
    principal_direction = eigenvectors[:, idx]
    del eigenvalues
    # 计算 model_re 在主特征向量方向上的投影
    proj = torch.matmul(principal_direction.unsqueeze(0), all_updates) * principal_direction.unsqueeze(1)
    grad_orthogonal = all_updates - proj
    #del proj
    # 选择与主方向正交的梯度分量进行参数更新，或者反向更新
    new_all_updates = -grad_orthogonal  # 取反向以尝试增加损失
    del grad_orthogonal
    #weights = calculate_angle_based_weights(all_updates,new_all_updates)
    #model_re = torch.mean(all_updates, dim=0)
    #new_all_updates -= weights * new_all_updates
    # 根据权重进一步调整已修改的梯度
    #for i in range(len(new_all_updates)):
        # 通过加强或减弱已修改梯度的权重来增加损失
        #direction = 1 if weights[i] > 0.5 else -1  # 如果权重较大，则增强该方向；否则，可能减弱
        #new_all_updates[i] += direction *  weights[i] * new_all_updates[i]  # 使用自身的修改梯度进行加权
    #new_all_updates = new_all_updates[torch.argmax(weights)]
    #lamda = torch.Tensor([3.0]).float().cuda()
    #threshold_diff = 1e-5
    #lamda_fail = lamda
    #lamda_succ = 0
    #if dev_type == 'unit_vec':
     #   deviation = model_re / torch.norm(model_re)  # unit vector, direction opposite to good direction
    #elif dev_type == 'sign':
     #   deviation = torch.sign(model_re)
    #elif dev_type == 'std':
     #   deviation = torch.std(all_updates, 0)
    #while torch.abs(lamda_succ - lamda) > threshold_diff:
     #   mal_update = new_all_updates - lamda * deviation
        # xiaolist = []
        # for xiao in mal_update:
      #  current_angle = calculate_cos_angle(mal_update, model_re)
            # xiaolist.append(current_angle)
        
     #   if current_angle > (torch.max(weights)):
      #      lamda_succ = lamda
       #     lamda = lamda + lamda_fail / 2
       # else:
        #    lamda = lamda - lamda_fail / 2

       # lamda_fail = lamda_fail / 2
    #mal_update = new_all_updates - lamda_succ * deviation
    # mean_vector = torch.mean(all_updates, dim=0)
    # average_direction = mean_vector / torch.norm(mean_vector)
    # angles_matrix = []
    # angles_matrix_2 = []
    # # grad = []
    # for update in all_updates:
    #     angle = calculate_cos_angle(update, average_direction)
    #     angles_matrix.append(angle)
    #
    # target_angle = max(angles_matrix)
    # i = 0
    # for model_re in new_all_updates:
    #     while torch.abs(lamda_succ - lamda) > threshold_diff:
    #         mal_update = model_re - lamda * deviation
    #         current_angle = calculate_cos_angle(mal_update, average_direction)
    #         if current_angle > (target_angle):
    #             lamda_succ = lamda
    #             lamda = lamda + lamda_fail / 2
    #         else:
    #             lamda = lamda - lamda_fail / 2
    #
    #         lamda_fail = lamda_fail / 2
    #
    #     mal_update = model_re - lamda_succ * deviation
    #     new_all_updates[i] = mal_update
    #     i += 1
    # angles_matrix=[]
    # for update in new_all_updates:
    #     angle = calculate_cos_angle(update, average_direction)
    #     angles_matrix.append(angle)
    # sorted_angles, indices = torch.sort(torch.tensor(angles_matrix), descending=True)
    # mal_update = new_all_updates[indices[1]]
    #gradient_norm = F.normalize(all_updates, dim=-1)
    #new_mean = torch.mean(new_all_updates,dim=0)
    #gradient_moved_norm = F.normalize(new_all_updates, dim=-1)
    #consis_constraint =  max(F.cosine_similarity(gradient_norm,new_mean, dim=0))
    #del gradient_norm
    # weight_moved = torch.exp(-torch.abs(proj)).reshape(-1,consis_constraint.shape[-1]) 
    
    #weight_moved = torch.tanh(torch.mean(proj,dim=0))
    #del proj
    #quan   = max(consis_constraint)
    #consis_constraint = F.normalize(consis_constraint , dim=-1)
    #consis_constraint = F.normalize(consis_constraint , dim=-1)
    #del all_updates
    # print(consis_constraint.shape)
    #if quan<0.5:
    #lamda = torch.Tensor([3.0]).float().cuda()
    #threshold_diff = 1e-5
    #lamda_fail = lamda
    #lamda_succ = 0
    #distances = []
    #for update in all_updates:
     #   distance = torch.norm((gradient_norm - update), dim=1) ** 2
      #  distances = distance[None, :] if not len(distances) else torch.cat((distances, distance[None, :]), 0)

    #max_distance = torch.max(distances)
    #del distances
    #while torch.abs(lamda_succ - lamda) > threshold_diff:
        #mal_update = new_mean - lamda * weight_moved
        # xiaolist = []
        # for xiao in mal_update:
        #current_angle = max(F.cosine_similarity(gradient_norm,mal_update, dim=0))
            # xiaolist.append(current_angle)
        #distance = torch.norm((gradient_norm - mal_update), dim=1) ** 2
        #max_d = torch.max(distance)
        
        #if max_d <= max_distance:
        #if torch.all(current_angle > consis_constraint):#if current_angle > (consis_constraint):
          #  lamda_succ = lamda
         #   lamda = lamda + lamda_fail / 2
        #else:
         #   lamda = lamda - lamda_fail / 2

        #lamda_fail = lamda_fail / 2
    #if lamda_succ ==0:
    #mal_update = new_mean - (1-consis_constraint) * weight_moved
    #else:
        ##mal_update = new_mean + (1-lamda_succ) * weight_moved
    #mal_update = torch.matmul(-consis_constraint, new_all_updates)
    #else:
     #   mal_update = torch.matmul(-consis_constraint, new_all_updates)+torch.mean(proj,dim=0)
    global pred_mal_update
    mal_update = torch.mean(new_all_updates, dim=0)
    del new_all_updates
    new_mean_all_updates = mal_update
    difference = torch.mean(mal_update,dim=0)
    mal_all_updates = torch.mean(mal_update, dim=0)
    
    deviation = difference / torch.norm(difference)
    del difference
    if pred_mal_update is not None:
        def orthogonal_projection(new_updates, pred_updates,mal_all_updates):
            unit_pred = pred_updates / torch.norm(pred_updates)
            best_scale = 0
            best_cosine = 1  # 初始化为最大可能的余弦相似度
            best_error_vector = None

    # 尝试不同的 scale 值以找到最小化余弦相似度的最优解
            for scale in torch.linspace(-10, 10, 200):  # 尝试从 -10 到 10 的 200 个不同的倍数
                projection = torch.dot(new_updates, unit_pred) * unit_pred * scale
                grad_orthogonal = all_updates - proj*scale
    # del proj
    # 选择与主方向正交的梯度分量进行参数更新，或者反向更新
                new_all_updates = -grad_orthogonal
                new_all_updates = torch.mean(new_all_updates, dim=0)
                error_vector = new_all_updates + projection
                desired_length = torch.norm(mal_all_updates)
                error_vector = error_vector / torch.norm(error_vector) * desired_length
                cosine_similarity = torch.cosine_similarity(error_vector.unsqueeze(0),pred_updates.unsqueeze(0))
        
                if cosine_similarity < best_cosine:
                    best_cosine = cosine_similarity
                    best_scale = scale
                    best_error_vector = error_vector
            #projection = torch.dot(new_updates, unit_pred) * unit_pred
            #error_vector = new_updates + projection
            #desired_length = torch.norm(mal_all_updates)
            #error_vector = error_vector / torch.norm(error_vector) * desired_length
            return best_error_vector
        error_vector = orthogonal_projection(new_mean_all_updates, pred_mal_update,mal_all_updates)
        del proj
        mal_update = new_mean_all_updates+error_vector
        cos_angle = calculate_cos_angle(mal_update, pred_mal_update)
    pred_mal_update = mal_update
    #del new_all_updates
    
    return mal_update


import torch
def compute_fisher_diagonal(gradients):
    # 初始化一个向量用于存储Fisher信息矩阵的对角元素
    fisher_diagonal = torch.zeros(gradients[0].shape).cuda()
    
    # 累加每个梯度向量元素的平方
    for grad in gradients:
        fisher_diagonal += grad ** 2
    
    # 将累加结果除以梯度向量的数量，得到平均值
    fisher_diagonal /= len(gradients)
    return fisher_diagonal
global fisher_matrix,num_data_points
fisher_matrix =None

def our_attack_hasson(all_updates, model_re, n_attackers, dev_type='unit_vec', alpha=0.7):
    # 假设 all_updates 是一个 [攻击者数量, 特征数量] 的张量
    # 计算平均梯度
    mean_vector = torch.mean(all_updates, dim=0)

    fim_diag_approx = torch.mean(all_updates.pow(2), dim=0)
    global fisher_matrix, num_data_points

    # 确保fisher_matrix是二维的
    if fisher_matrix is None:
        fisher_matrix = fim_diag_approx
        num_data_points = 1  # 初始化样本数
    else:
        num_data_points += 1  # 更新样本数
        # 计算新的平均值
        fisher_matrix = fisher_matrix + (fim_diag_approx - fisher_matrix) / num_data_points

    #H = -fisher_matrix
    # print(H.shap
    # 使用对角矩阵的逆进行更新
    #mal_update = -mean_vector*H-mean_vector*fim_diag_approx
    mean_vector = torch.mean(all_updates, dim=1, keepdim=True)

    # 中心化梯度矩阵
    centered_matrix = all_updates - mean_vector

    # 计算协方差矩阵
    covariance_matrix = centered_matrix @ centered_matrix.t() / (all_updates.size(1) - 1)
    del centered_matrix
    # 计算标准差
    std_dev = torch.sqrt(torch.diag(covariance_matrix))

    # 从协方差矩阵计算相关矩阵
    correlation_matrix = covariance_matrix / torch.outer(std_dev, std_dev)
    del covariance_matrix,std_dev
    # 计算特征值和特征向量
    eigenvalues, eigenvectors = torch.linalg.eigh(correlation_matrix)
    del correlation_matrix
    # 找到最大特征值对应的特征向量
    max_eigenvalue, idx = torch.max(eigenvalues, dim=0)
    principal_direction = eigenvectors[:, idx]
    del eigenvalues
    # 计算 all_updates 在主特征向量方向上的投影
    proj = torch.matmul(principal_direction.unsqueeze(0), all_updates) * principal_direction.unsqueeze(1)
    # proj = torch.matmul(principal_direction.unsqueeze(0), all_updates) / torch.dot(principal_direction, principal_direction) * principal_direction
    # 计算与主特征向量正交的梯度分量

    grad_orthogonal = all_updates - proj
    # del proj
    # 选择与主方向正交的梯度分量进行参数更新，或者反向更新
    new_all_updates = -grad_orthogonal  # 取反向以尝试增加损失,3个节点
    #new_all_updates = torch.mean(new_all_updates, dim=0)
    fim_diag_approx_new = torch.mean(all_updates.pow(2), dim=0)
    mean_vector = torch.mean(new_all_updates, dim=0)
    #mal_update = -new_all_updates*fim_diag_approx_new-mean_vector*fisher_matrix
    # print(mal_update.shape)
    mal_update_s=[]
    deviation = torch.std(new_all_updates, 0)

    # 初始化变量
    norm_value = 0
    max_value = None
    data_max = None
    app =[]
    #norm_value = float('inf')

    for data_e in all_updates:
        # 计算FIM的对角线近似
        fim_diag_approx = torch.mean(data_e.pow(2), dim=0)
        #print(fim_diag_approx)
        app.append(data_e/(fim_diag_approx+ 1e-6))
        # 计算FIM对角线近似的最大范数（无穷范数）
        norm = torch.norm(fim_diag_approx, float('inf')).item()  # 使用最大范数

        # 检查是否是最大范数
        if norm > norm_value:
            norm_value = norm
            max_value = fim_diag_approx
            data_max =data_e
        # 计算FIM对角线近似的最大范数（无穷范数）
    app = torch.stack(app).cuda()
    #new =model_re-0.5*torch.mean(app)*mean_vector
    new = model_re+0.01*(torch.mean(all_updates)/torch.mean(app))
    #print(principal_direction.shape)
    #print(fim_diag_approx_new.shape)
    #new = -fim_diag_approx_new*(principal_direction@new_all_updates)
    #print(new.shape)
    #mean_vector = torch.squeeze(torch.mean(all_updates, dim=0))
    #new =-max_value*new_all_updates
    return torch.squeeze(new)
def find_lambda_success(model_re, f, model_mean,obj_angle):
    for lamda in np.arange(1, -1, -0.01):
        mal_update = model_re - lamda * f
        current = calculate_cos_angle(mal_update,model_re)
        if current>obj_angle:
            return lamda, mal_update  # 成功的 lambda 和相应的恶意更新
    mal_update = model_re - 0.001 * f
    return 0.001, mal_update  # 如果没有找到成功的 lambda
global predfim
predfim=None
def our_attack_hasson1(all_updates, model_re):
    app = []
    fim_diag_approx_list =[]
    target_mean = torch.mean(all_updates,dim=0)
    target_std = torch.std(all_updates,dim=0)
    fim_diag_approx_target_mean = target_mean.pow(2)
    global predfim
    for index,data_e in enumerate(all_updates):
        # 计算FIM的对角线近似
        fim_diag_approx = data_e.pow(2)
        fim_diag_approx_list.append(fim_diag_approx)
        if predfim is None:
            new_apps = (data_e/(fim_diag_approx+1e-20))@data_e
            #new_app = data_e+0.01*data_e/(data_e/(fim_diag_approx+ 1e-6))
        else:
            predfim_i = predfim[index]
            fim_diag_approx_new = fim_diag_approx -predfim_i
            
        #new_app = data_e+0.01*data_e/(data_e/(fim_diag_approx+ 1e-6))
    
            new_apps = (data_e/(fim_diag_approx-predfim_i+1e-20))@data_e
        #new_apps = data_e+0.01*data_e/(data_e/(fim_diag_approx+1e-10)+1e-10)
        print(calculate_cos_angle(new_apps,target_mean))
        print(torch.norm((all_updates -new_apps ), dim=1) ** 2)
        app.append(new_apps)
    fim_diag_approx_list = torch.stack(fim_diag_approx_list)
    predfim = fim_diag_approx_list
    app = torch.stack(app)
    #app_new = torch.mean(app,dim=0)
    #print(app_new.shape)
    #fim_diag_approx_list = torch.stack(fim_diag_approx_list).cuda()
    #new = model_re+0.01*(model_re/torch.mean(app))
    
    return app
def our_attack_hessanangle(all_updates, model_re, n_attackers, dev_type='unit_vec'):
    if dev_type == 'unit_vec':
        deviation = model_re / torch.norm(model_re)  # unit vector, direction opposite to good direction
    elif dev_type == 'sign':
        deviation = torch.sign(model_re)
    elif dev_type == 'std':
        deviation = torch.std(all_updates, 0)

    lamda = torch.Tensor([10.0]).float().cuda()
    threshold_diff = 1e-5
    lamda_fail = lamda
    lamda_succ = 0
    mean_vector = torch.mean(all_updates, dim=0)
    fim_diag_mean = mean_vector.pow(2)
    app=[]
    for data_e in all_updates:
        fim_diag_approx = data_e.pow(2)
        target_angle = calculate_cos_angle(fim_diag_approx, fim_diag_mean)

        app.append(target_angle)
    target_angle = calculate_cos_angle(all_updates[app.index(min(app))], mean_vector)
    model_re =model_re+0.5*torch.matmul(-fim_diag_mean, mean_vector)
    while torch.abs(lamda_succ - lamda) > threshold_diff:
        mal_update = model_re - lamda * deviation
        current_angle = calculate_cos_angle(mal_update, mean_vector)
        if current_angle > (target_angle):
            lamda_succ = lamda
            lamda = lamda + lamda_fail / 2
        else:
            lamda = lamda - lamda_fail / 2

        lamda_fail = lamda_fail / 2
    mal_update = model_re - lamda_succ * deviation

    return mal_update
def our_attack_hessanangle_mean(all_updates, model_re, n_attackers, dev_type='unit_vec'):
    if dev_type == 'unit_vec':
        deviation = model_re / torch.norm(model_re)  # unit vector, direction opposite to good direction
    elif dev_type == 'sign':
        deviation = torch.sign(model_re)
    elif dev_type == 'std':
        deviation = torch.std(all_updates, 0)

    lamda = torch.Tensor([10.0]).float().cuda()
    threshold_diff = 1e-5
    lamda_fail = lamda
    lamda_succ = 0
    mean_vector = torch.mean(all_updates, dim=0)
    # fim_diag_mean = mean_vector.pow(2)

    app=[]
    fim_list = []
    for data_e in all_updates:
        fim_diag_approx = data_e.pow(2)
        fim_list.append(fim_diag_approx)
    fim_list = torch.stack(fim_list).cuda()
    fim_diag_mean = torch.mean(fim_list,dim=0)
    model_re = model_re +0.01*torch.matmul(-fim_diag_mean, mean_vector)
    for li in fim_list:
        target_angle = calculate_cos_angle(li, fim_diag_mean)
        app.append(target_angle)

    target_angle = calculate_cos_angle(all_updates[app.index(min(app))], mean_vector)

    while torch.abs(lamda_succ - lamda) > threshold_diff:
        mal_update = model_re - lamda * deviation
        current_angle = calculate_cos_angle(mal_update, mean_vector)
        if current_angle > (target_angle):
            lamda_succ = lamda
      #      print(lamda_succ)
            lamda = lamda + lamda_fail / 2
        else:
            lamda = lamda - lamda_fail / 2

        lamda_fail = lamda_fail / 2
    mal_update = model_re - lamda_succ * deviation

    return mal_update

def calculate_cos_angle(a, b):
    a_flat = a.view(-1)
    b_flat = b.view(-1)
    cos_angle = torch.dot(a_flat, b_flat) / (torch.norm(a_flat) * torch.norm(b_flat))
    return cos_angle


def our_attack_dist_second(all_updates, model_re, n_attackers, dev_type='unit_vec'):
    if dev_type == 'unit_vec':
        deviation = model_re / torch.norm(model_re)  # unit vector, direction opposite to good direction
    elif dev_type == 'sign':
        deviation = torch.sign(model_re)
    elif dev_type == 'std':
        deviation = torch.std(all_updates, 0)

    lamda = torch.Tensor([10.0]).float().cuda()
    threshold_diff = 1e-5
    lamda_fail = lamda
    lamda_succ = 0
    
    # Calculate the average cosine angle between the model_re and all updates as the target angle
    angles = []
    mean_vector = torch.mean(all_updates, dim=0)

    # 归一化平均向量以得到单位向量
    average_direction = mean_vector / torch.norm(mean_vector)
    #for update in all_updates:
      #  angle = calculate_cos_angle(update, average_direction)
     #   angles.append(angle)
    #angles = torch.tensor(angles).unsqueeze(0).cuda() 
    #C = torch.matmul(angles.T, angles)  # 现在 angles 是 [10, 1]，所以 angles.T * angles 将是 [10, 10]

    # 计算特征值和特征向量
    #eigenvalues, eigenvectors = torch.linalg.eig(C)

    # 处理实数部分
    #real_eigenvalues = eigenvalues.real
    #eigenvectors = eigenvectors.real

    # 找到最小特征值对应的最小特征向量
    #min_eigenvalue_index = torch.argmax(real_eigenvalues)
    #min_eigenvector = eigenvectors[:, min_eigenvalue_index]
    #min_eigenvector_index = torch.argmin(min_eigenvector)
    #target_angle_value = F.sigmoid(angles.squeeze()[min_eigenvector_index])
    #model_re = model_re - target_angle_value * deviation
    #target_angle = sorted_angles[0]
    angles_matrix = []

    # 外层循环遍历所有向量
    for i, update_vec_i in enumerate(all_updates):
        angles = []
        # 内层循环遍历其他所有向量
        for j, update_vec_j in enumerate(all_updates):
            if i != j:  # 确保不与自身比较
                cos_angle = calculate_cos_angle(update_vec_i, update_vec_j)
                # print(cos_angle)
                angles.append(cos_angle.item())  # 存储角度
            else:
                angles.append(0)  # 自身与自身的角度为0
        angles_matrix.append(angles)
        del angles
    # 将 angles_matrix 列表转换为张量
    angles_matrix_tensor = torch.tensor(angles_matrix, dtype=torch.float32).cuda()
    new_A = torch.matmul(angles_matrix_tensor, all_updates)
    angles_matrix_new = []

    # 外层循环遍历所有向量
    for i, update_vec_i in enumerate(new_A):
        angles = []
        # 内层循环遍历其他所有向量
        for j, update_vec_j in enumerate(all_updates):
            if i != j:  # 确保不与自身比较
                cos_angle = calculate_cos_angle(update_vec_i, update_vec_j)
                # print(cos_angle)
                angles.append(cos_angle.item())  # 存储角度
            else:
                angles.append(0)  # 自身与自身的角度为0
        angles_matrix_new.append(angles)
        del angles
    #del new_A
    new_A = torch.matmul(angles_matrix_tensor, all_updates)
    angles_matrix_new = []

    # 外层循环遍历所有向量
    for i, update_vec_i in enumerate(new_A):
        angles = []
        # 内层循环遍历其他所有向量
        for j, update_vec_j in enumerate(all_updates):
            if i != j:  # 确保不与自身比较
                cos_angle = calculate_cos_angle(update_vec_i, update_vec_j)
                # print(cos_angle)
                angles.append(cos_angle.item())  # 存储角度
            else:
                angles.append(0)  # 自身与自身的角度为0
        angles_matrix_new.append(angles)
        del angles
    #del new_A
    angles_matrix_tensor_new = torch.tensor(angles_matrix_new, dtype=torch.float32).cuda()
        # 使用torch.linalg.eig计算特征值和特征向量
    eigenvalues, eigenvectors = torch.linalg.eig(angles_matrix_tensor_new)

    max_eigenvalue, max_index = torch.max(eigenvalues.real, 0)
    max_eigenvector = eigenvectors[:, max_index].real
    # print(max_eigenvector.shape)
    new_max_tensor = torch.matmul(max_eigenvector, new_A)
    target_angle = calculate_cos_angle(new_max_tensor, average_direction)


    while torch.abs(lamda_succ - lamda) > threshold_diff:
        mal_update = model_re - lamda * deviation
        current_angle = calculate_cos_angle(mal_update, average_direction)
        if current_angle > (target_angle):
            lamda_succ = lamda
            lamda = lamda + lamda_fail / 2
        else:
            lamda = lamda - lamda_fail / 2

        lamda_fail = lamda_fail / 2

    mal_update = model_re - lamda_succ * deviation
    
    return mal_update
def our_attack_min_eigenvalue(all_updates, model_re, n_attackers, dev_type='unit_vec'):
    if dev_type == 'unit_vec':
        deviation = model_re / torch.norm(model_re)  # unit vector, direction opposite to good direction
    elif dev_type == 'sign':
        deviation = torch.sign(model_re)
    elif dev_type == 'std':
        deviation = torch.std(all_updates, 0)
    angles = []
    mean_vector = torch.mean(all_updates, dim=0)

    # 归一化平均向量以得到单位向量
    # average_direction = mean_vector / torch.norm(mean_vector)
    for update in all_updates:
        angle = calculate_cos_angle(update, deviation)
        angles.append(angle)
    sorted_angles, indices = torch.sort(torch.tensor(angles), descending=True)
    angles = torch.tensor(angles).unsqueeze(0).cuda() 
    C = torch.matmul(angles.T, angles)  # 现在 angles 是 [10, 1]，所以 angles.T * angles 将是 [10, 10]

    # 计算特征值和特征向量
    eigenvalues, eigenvectors = torch.linalg.eig(C)

    # 处理实数部分
    real_eigenvalues = eigenvalues.real
    eigenvectors = eigenvectors.real

    # 找到最小特征值对应的最小特征向量
    min_eigenvalue_index = torch.argmin(real_eigenvalues)
    min_eigenvector = eigenvectors[:, min_eigenvalue_index]
    min_eigenvector_index = torch.argmin(min_eigenvector)
    target_angle = F.sigmoid(angles.squeeze()[min_eigenvector_index])

    mal_update = model_re - target_angle * deviation
    return mal_update


#Defense code

# code from https://github.com/vrt1shjwlkr/NDSS21-Model-Poisoning/blob/main/cifar10/release-fedsgd-alexnet-bulyan-unknown-benign-gradients.ipynb
def krum(all_updates, n_attackers, multi_k=False):
    candidates = []
    candidate_indices = []

    remaining_updates = all_updates
    all_indices = np.arange(len(all_updates))

    while len(remaining_updates) > 2 * n_attackers + 2:
        torch.cuda.empty_cache()
        distances = []
        for update in remaining_updates:
            distance = []
            for update_ in remaining_updates:
                distance.append(torch.norm((update - update_)) ** 2)
            distance = torch.Tensor(distance).float()
            distances = distance[None, :] if not len(distances) else torch.cat((distances, distance[None, :]), 0)

        distances = torch.sort(distances, dim=1)[0]
        scores = torch.sum(distances[:, :len(remaining_updates) - 2 - n_attackers], dim=1)
        indices = torch.argsort(scores)[:len(remaining_updates) - 2 - n_attackers]

        candidate_indices.append(all_indices[indices[0].cpu().numpy()])
        all_indices = np.delete(all_indices, indices[0].cpu().numpy())
        candidates = remaining_updates[indices[0]][None, :] if not len(candidates) else torch.cat((candidates, remaining_updates[indices[0]][None, :]), 0)

        remaining_updates = torch.cat((remaining_updates[:indices[0]], remaining_updates[indices[0] + 1:]), 0)
        if not multi_k:
            break

    aggregate = torch.mean(candidates, dim=0)


    return aggregate, np.array(candidate_indices)

def Multi_krum(all_updates, n_attackers, multi_k=True):

    candidates = []
    candidate_indices = []
    remaining_updates = all_updates
    all_indices = np.arange(len(all_updates))

    while len(remaining_updates) > 2 * n_attackers + 2:
        torch.cuda.empty_cache()
        distances = []
        for update in remaining_updates:
            distance = []
            for update_ in remaining_updates:
                distance.append(torch.norm((update - update_)) ** 2)
            distance = torch.Tensor(distance).float()
            distances = distance[None, :] if not len(distances) else torch.cat((distances, distance[None, :]), 0)

        distances = torch.sort(distances, dim=1)[0]
        scores = torch.sum(distances[:, :len(remaining_updates) - 2 - n_attackers], dim=1)
        indices = torch.argsort(scores)[:len(remaining_updates) - 2 - n_attackers]

        candidate_indices.append(all_indices[indices[0].cpu().numpy()])
        all_indices = np.delete(all_indices, indices[0].cpu().numpy())
        candidates = remaining_updates[indices[0]][None, :] if not len(candidates) else torch.cat((candidates, remaining_updates[indices[0]][None, :]), 0)
        remaining_updates = torch.cat((remaining_updates[:indices[0]], remaining_updates[indices[0] + 1:]), 0)
        if not multi_k:
            break

    if isinstance(candidates, list):
        candidates_tensor = torch.stack(candidates)  # Convert list of tensors to a single tensor
        aggregate = torch.mean(candidates_tensor, dim=0)
    else:
        # If candidates is already a tensor
        aggregate = torch.mean(candidates, dim=0)

    return aggregate, np.array(candidate_indices)

def Bulyan(all_updates, n_attackers):
    nusers = all_updates.shape[0]
    bulyan_cluster = []
    candidate_indices = []
    remaining_updates = all_updates
    all_indices = np.arange(len(all_updates))

    while len(bulyan_cluster) < (nusers - 2 * n_attackers):
        torch.cuda.empty_cache()
        distances = []
        for update in remaining_updates:
            distance = []
            for update_ in remaining_updates:
                distance.append(torch.norm((update - update_)) ** 2)
            distance = torch.Tensor(distance).float()
            distances = distance[None, :] if not len(distances) else torch.cat((distances, distance[None, :]), 0)

        distances = torch.sort(distances, dim=1)[0]

        scores = torch.sum(distances[:, :len(remaining_updates) - 2 - n_attackers], dim=1)
        indices = torch.argsort(scores)[:len(remaining_updates) - 2 - n_attackers]
        if not len(indices):
            break
        candidate_indices.append(all_indices[indices[0].cpu().numpy()])
        all_indices = np.delete(all_indices, indices[0].cpu().numpy())
        bulyan_cluster = remaining_updates[indices[0]][None, :] if not len(bulyan_cluster) else torch.cat((bulyan_cluster, remaining_updates[indices[0]][None, :]), 0)
        remaining_updates = torch.cat((remaining_updates[:indices[0]], remaining_updates[indices[0] + 1:]), 0)


    n, d = bulyan_cluster.shape
    param_med = torch.median(bulyan_cluster, dim=0)[0]
    sort_idx = torch.argsort(torch.abs(bulyan_cluster - param_med), dim=0)
    sorted_params = bulyan_cluster[sort_idx, torch.arange(d)[None, :]]

    return torch.mean(sorted_params[:n - 2 * n_attackers], dim=0), np.array(candidate_indices)


def Median(w):
    w_avg = torch.median(w, dim=0)[0]
    return w_avg

def tr_mean(all_updates, n_attackers):
    sorted_updates = torch.sort(all_updates, 0)[0]
    out = torch.mean(sorted_updates[n_attackers:-n_attackers], 0) if n_attackers else torch.mean(sorted_updates,0)
    return out

def FedAvg(w):

    w_avg= torch.mean(w, dim=0)
    return w_avg

def Norm_bounding(w):
    #w 50*2472266
    per_norm=torch.norm(w, p=2,dim=1)
    avg_norm=torch.mean(per_norm,dim=0)
    n=per_norm/avg_norm
    mat=torch.max(torch.ones(size=n.size()).cuda(),n)
    mat=mat.reshape(-1,1)
    ans=torch.reciprocal(mat)*w
    return torch.mean(ans, dim=0)

def train_attack_Defense(at_type='adjust_lamda_for_target_cos_angle',aggregation='bulyan',n_attackers=[10],arch='alexnet',seed = 42):
    
    reset_seed(seed)
    reset_torch_state()
    print('now use seed:',seed,'at_type:',at_type)
    data_loc='./cifar10_data/'
    
    train_transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
        ])
    
    cifar10_train = datasets.CIFAR10(root=data_loc, train=True, download=True, transform=train_transform)
    
    cifar10_test = datasets.CIFAR10(root=data_loc, train=False, download=True, transform=train_transform)
    
    X=[]
    Y=[]
    for i in range(len(cifar10_train)):
        X.append(cifar10_train[i][0].numpy())
        Y.append(cifar10_train[i][1])
    
    for i in range(len(cifar10_test)):
        X.append(cifar10_test[i][0].numpy())
        Y.append(cifar10_test[i][1])
    
    X=np.array(X)
    Y=np.array(Y)
    
    print('total data len: ',len(X))
    
    if not os.path.isfile('./cifar10_shuffle.pkl'):
        all_indices = np.arange(len(X))
        np.random.shuffle(all_indices)
        pickle.dump(all_indices,open('./cifar10_shuffle.pkl','wb'))
    else:
        all_indices=pickle.load(open('./cifar10_shuffle.pkl','rb'))
    
    X=X[all_indices]
    Y=Y[all_indices]
    # data loading
    
    nusers=50
    user_tr_len=1000
    
    total_tr_len=user_tr_len*nusers
    val_len=5000
    te_len=5000
    
    print('total data len: ',len(X))
    
    if not os.path.isfile('./cifar10_shuffle.pkl'):
        all_indices = np.arange(len(X))
        np.random.shuffle(all_indices)
        pickle.dump(all_indices,open('./cifar10_shuffle.pkl','wb'))
    else:
        all_indices=pickle.load(open('./cifar10_shuffle.pkl','rb'))
    
    total_tr_data=X[:total_tr_len]
    total_tr_label=Y[:total_tr_len]
    
    val_data=X[total_tr_len:(total_tr_len+val_len)]
    val_label=Y[total_tr_len:(total_tr_len+val_len)]
    
    te_data=X[(total_tr_len+val_len):(total_tr_len+val_len+te_len)]
    te_label=Y[(total_tr_len+val_len):(total_tr_len+val_len+te_len)]
    
    total_tr_data_tensor=torch.from_numpy(total_tr_data).type(torch.FloatTensor)
    total_tr_label_tensor=torch.from_numpy(total_tr_label).type(torch.LongTensor)
    
    val_data_tensor=torch.from_numpy(val_data).type(torch.FloatTensor)
    val_label_tensor=torch.from_numpy(val_label).type(torch.LongTensor)
    
    te_data_tensor=torch.from_numpy(te_data).type(torch.FloatTensor)
    te_label_tensor=torch.from_numpy(te_label).type(torch.LongTensor)
    
    print('total tr len %d | val len %d | test len %d'%(len(total_tr_data_tensor),len(val_data_tensor),len(te_data_tensor)))
    
    #==============================================================================================================
    
    user_tr_data_tensors=[]
    user_tr_label_tensors=[]
    
    for i in range(nusers):
    
        user_tr_data_tensor=torch.from_numpy(total_tr_data[user_tr_len*i:user_tr_len*(i+1)]).type(torch.FloatTensor)
        user_tr_label_tensor=torch.from_numpy(total_tr_label[user_tr_len*i:user_tr_len*(i+1)]).type(torch.LongTensor)
    
        user_tr_data_tensors.append(user_tr_data_tensor)
        user_tr_label_tensors.append(user_tr_label_tensor)
        print('user %d tr len %d'%(i,len(user_tr_data_tensor)))
    batch_size=250
    
    resume=0
    nepochs=1200
    schedule=[1000]
    nbatches = user_tr_len//batch_size

    gamma=.5
    opt = 'sgd'
    fed_lr=0.5
    criterion=nn.CrossEntropyLoss()
    use_cuda = torch.cuda.is_available()

    candidates = []

    dev_type ='std'
    chkpt='./' + aggregation + at_type + str(n_attackers[0])
    # chkpt='./'+aggregation+at_type+n_attackers

    for n_attacker in n_attackers:

        candidates = []

        epoch_num = 0
        best_global_acc = 0
        best_global_te_acc = 0

        fed_model, _ = return_model(arch, 0.1, 0.9, parallel=False)
        optimizer_fed = SGD(fed_model.parameters(), lr=fed_lr)

        torch.cuda.empty_cache()
        r=np.arange(user_tr_len)

        while epoch_num <= nepochs:
            user_grads=[]
            if not epoch_num and epoch_num%nbatches == 0:
                np.random.shuffle(r)
                for i in range(nusers):
                    user_tr_data_tensors[i]=user_tr_data_tensors[i][r]
                    user_tr_label_tensors[i]=user_tr_label_tensors[i][r]

            for i in range(nusers):

                inputs = user_tr_data_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]
                targets = user_tr_label_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]

                inputs, targets = inputs.cuda(), targets.cuda()
                inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)

                outputs = fed_model(inputs)
                loss = criterion(outputs, targets)
                fed_model.zero_grad()
                loss.backward(retain_graph=True)

                param_grad=[]
                for param in fed_model.parameters():
                    param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))

                user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)

            if epoch_num in schedule:
                for param_group in optimizer_fed.param_groups:
                    param_group['lr'] *= gamma
                    print('New learnin rate ', param_group['lr'])

            if n_attacker > 0:
                n_attacker_ = max(1, n_attacker**2//nusers)
                if at_type == 'lie_attack':
                    z_values={5:0.69847, 10:0.7054, 15:0.71904, 20:0.72575, 12:0.73891}
                    mal_update = lie_attack(user_grads[:n_attacker], z_values[n_attacker])

                elif at_type == 'Shejwalkar_attack_mkrum' and (aggregation == 'Multi_krum' or aggregation == 'krum'):
                    agg_grads = torch.mean(user_grads[:n_attacker], 0)
                    mal_update = Shejwalkar_attack_mkrum(user_grads[:n_attacker], agg_grads, n_attacker_, dev_type)
                elif at_type == 'Shejwalkar_attack_min_max':
                    agg_grads = torch.mean(user_grads[:n_attacker], 0)
                    mal_update = Shejwalkar_attack_min_max(user_grads[:n_attacker], agg_grads, n_attacker_, dev_type)
                elif at_type == 'Shejwalkar_attack_min_sum':
                    agg_grads = torch.mean(user_grads[:n_attacker], 0)
                    mal_update = Shejwalkar_attack_min_sum(user_grads[:n_attacker], agg_grads, n_attacker_, dev_type)
                elif at_type == 'adjust_lamda_for_target_cos_angle':
                    agg_grads = torch.mean(user_grads[:n_attacker], 0)
                    mal_update = adjust_lamda_for_target_cos_angle(user_grads[:n_attacker], agg_grads, n_attacker_, dev_type)
                elif at_type == 'fang_attack_trmean_partial' and aggregation == 'tr_mean':
                    agg_grads = torch.mean(user_grads[:n_attacker], 0)
                    mal_updates = fang_attack_trmean_partial(user_grads[:n_attacker],n_attacker)
                elif at_type == 'fang_attack_bulyan_partial' and aggregation == 'Bulyan':
                    agg_grads = torch.mean(user_grads[:n_attacker], 0)
                    deviation = torch.sign(agg_grads)
                    mal_update = fang_attack_bulyan_partial(user_grads[:n_attacker], agg_grads,deviation, n_attacker_)
                elif at_type == 'fang_attack_krum_partial' and (aggregation == 'Multi_krum' or aggregation == 'krum'):
                    agg_grads = torch.mean(user_grads[:n_attacker], 0)
                    deviation = torch.sign(agg_grads)
                    mal_update = fang_attack_krum_partial(user_grads[:n_attacker], agg_grads,deviation, n_attacker_)
                elif at_type == 'our_attack_dist_second':
                    agg_grads = torch.mean(user_grads[:n_attacker], 0)
                    mal_update = our_attack_dist_second(user_grads[:n_attacker], agg_grads, n_attacker_, dev_type)
                elif at_type == 'our_attack_hasson':
                    agg_grads = torch.mean(user_grads[:n_attacker], 0)
                    mal_updates = our_attack_hasson1(user_grads[:n_attacker], agg_grads)
                elif at_type == 'our_attack_hessanangle':
                    agg_grads = torch.mean(user_grads[:n_attacker], 0)
                    mal_update = our_attack_hessanangle(user_grads[:n_attacker], agg_grads, n_attacker_, dev_type)
                elif at_type == 'our_attack_hessanangle_mean':
                    agg_grads = torch.mean(user_grads[:n_attacker], 0)
                    mal_update = our_attack_hessanangle_mean(user_grads[:n_attacker], agg_grads, n_attacker_, dev_type)
                elif at_type == 'our_attack_proj':
                    agg_grads = torch.mean(user_grads[:n_attacker], 0)
                    mal_update = our_attack_proj(user_grads[:n_attacker], agg_grads, n_attacker_, dev_type)
                if at_type == 'fang_attack_trmean_partial' and aggregation == 'tr_mean':
                    # mal_updates = mal_update/our_attack_min_eigenvalue
                    pass
                elif at_type == 'our_attack_hasson':
                    pass

                else:
                    mal_updates = torch.stack([mal_update] * n_attacker)
                malicious_grads = torch.cat((mal_updates, user_grads[n_attacker:]), 0)
            if not (malicious_grads.shape[0]==50):
                print(malicious_grads.shape)
                sys.exit()

            if aggregation == 'krum':
                agg_grads, krum_candidate = krum(malicious_grads, n_attacker)
                # print('end',aggregation, at_type,n_attacker)
                # break
            elif aggregation == 'Multi_krum':
                agg_grads, krum_candidate = Multi_krum(malicious_grads, n_attacker)
                # print('end',aggregation, at_type,n_attacker)
                # break
            elif aggregation == 'Bulyan':
                agg_grads, krum_candidate = Bulyan(malicious_grads, n_attacker)
                # print('end',aggregation, at_type,n_attacker)
                # break
            elif aggregation == 'Median':
                agg_grads = Median(malicious_grads)
                # print('end',aggregation, at_type,n_attacker)
                # break
            elif aggregation == 'tr_mean':
                agg_grads = tr_mean(malicious_grads, n_attacker)
                # print('end',aggregation, at_type,n_attacker)
                # break
            elif aggregation == 'FedAvg':
                agg_grads = FedAvg(malicious_grads)
                # print('end',aggregation, at_type,n_attacker)
                # break
            elif aggregation == 'Norm_bounding':
                agg_grads = Norm_bounding(malicious_grads)
                # print('end',aggregation, at_type,n_attacker)
                # break


            del user_grads

            start_idx=0

            optimizer_fed.zero_grad()

            model_grads=[]

            for i, param in enumerate(fed_model.parameters()):
                param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)
                start_idx=start_idx+len(param.data.view(-1))
                param_=param_.cuda()
                model_grads.append(param_)

            optimizer_fed.step(model_grads)

            val_loss, val_acc = test(val_data_tensor,val_label_tensor,fed_model,criterion,use_cuda)
            te_loss, te_acc = test(te_data_tensor,te_label_tensor, fed_model, criterion, use_cuda)

            is_best = best_global_acc < val_acc

            best_global_acc = max(best_global_acc, val_acc)

            if is_best:
                best_global_te_acc = te_acc
            if aggregation == 'krum' or aggregation == 'Multi_krum' or aggregation == 'Bulyan':
                print('%s: at %s n_at %d n_mal_sel %d e %d | val loss %.4f val acc %.4f best val_acc %f' % (
                aggregation, at_type, n_attacker, np.sum(krum_candidate < n_attacker), epoch_num, val_loss, val_acc,
                best_global_acc))
            else:
                print('%s: at %s n_at %d  e %d | val loss %.4f val acc %.4f best val_acc %f' % (
                aggregation, at_type, n_attacker, epoch_num, val_loss, val_acc, best_global_acc))

            # print('%s: at %s n_at %d  e %d | val loss %.4f val acc %.4f best val_acc %f'%(aggregation, at_type, n_attacker, epoch_num, val_loss, val_acc, best_global_acc))

            if epoch_num%25==0 or epoch_num==nepochs-1:
                if aggregation == 'krum' or aggregation == 'Multi_krum' or aggregation == 'Bulyan':
                    print('+++++%s: at %s n_at %d n_mal_sel %d e %d | val loss %.4f val acc %.4f best val_acc %f'%(aggregation, at_type, n_attacker, np.sum(krum_candidate < n_attacker), epoch_num, val_loss, val_acc, best_global_acc))
                else:
                    print('+++++%s: at %s n_at %d  e %d | val loss %.4f val acc %.4f best val_acc %f'%(aggregation, at_type, n_attacker, epoch_num, val_loss, val_acc, best_global_acc))

            if val_loss > 1000:
                print('val loss %f too high'%val_loss)
                print('change new seed','now epoch:',epoch_num)
                break
            # Save checkpoints
            save_checkpoint_global({
                'epoch': epoch_num + 1,
                'state_dict': fed_model.state_dict(),
                'best_global_acc': best_global_acc,
                'best_global_te_acc': best_global_te_acc,
                'optimizer': optimizer_fed.state_dict(),
            }, is_best, checkpoint=chkpt)
            if aggregation == 'krum' or aggregation == 'Multi_krum' or aggregation == 'Bulyan':
                write_csv('training_progress_%s_%s_%s.csv'%(aggregation, at_type,n_attacker), [epoch_num, val_loss, val_acc, best_global_acc, te_acc, np.sum(krum_candidate < n_attacker)], header=(epoch_num == 0))
            else:
                write_csv('training_progress_%s_%s_%s.csv'%(aggregation, at_type,n_attacker), [epoch_num, val_loss, val_acc, best_global_acc, te_acc], header=(epoch_num == 0))
            

            epoch_num+=1
    return best_global_acc

# Defense = ['krum', 'Multi_krum', 'Bulyan', 'Median', 'tr_mean', 'FedAvg', 'Norm_bounding']
# AGR = ['fang_attack_krum_partial', 'fang_attack_bulyan_partial', 'Shejwalkar_attack_mkrum', 'fang_attack_trmean_partial']
# agr_agnostic = ['lie_attack', 'Shejwalkar_attack_min_max', 'Shejwalkar_attack_min_sum', 'adjust_lamda_for_target_cos_angle','our_attack_min_eigenvalue']


# attack_defense_mapping = {
#     'fang_attack_krum_partial': ['krum'],
#     'Shejwalkar_attack_mkrum': ['Multi_krum'],
#     'fang_attack_bulyan_partial': ['Bulyan'],
#     'fang_attack_trmean_partial': ['tr_mean'],

# }
from concurrent.futures import ProcessPoolExecutor, as_completed
# from multiprocessing import Manager
from multiprocessing import Process, Manager
          
import pandas as pd


def train_attack_Defense_wrapper(results, at_type, defense, n_attackers,seed_number):

    best_global_acc = train_attack_Defense(at_type=at_type, aggregation=defense, n_attackers=[n_attackers],seed=seed_number)
    results.append((at_type, defense, n_attackers, best_global_acc))

def run_experiments():
    Defense = ['Multi_krum','Norm_bounding','Bulyan', 'Median', 'tr_mean', 'FedAvg','krum']
    #AGR = ['fang_attack_krum_partial', 'fang_attack_bulyan_partial', 'Shejwalkar_attack_mkrum', 'fang_attack_trmean_partial']
    agr_agnostic = ['our_attack_hessanangle_mean']#['our_attack_hasson']#,'Shejwalkar_attack_min_max', 'Shejwalkar_attack_min_sum']#'our_attack_dist_second','Shejwalkar_attack_min_max', 'Shejwalkar_attack_min_sum']
    seed_number =123
    
    
    
    attack_defense_mapping = {
        'fang_attack_krum_partial': ['krum'],
        'Shejwalkar_attack_mkrum': ['Multi_krum'],
        'fang_attack_bulyan_partial': ['Bulyan'],
        'fang_attack_trmean_partial': ['tr_mean'],
    
    }

    with Manager() as manager:
        results = manager.list()  # 使用Manager来管理跨进程的列表

        for n_attackers in [10]:  # 指定攻击者数量
            for at_type in agr_agnostic:  # 遍历所有攻击类型
                for defense in Defense:  # 遍历所有防御策略
                    if at_type in attack_defense_mapping and defense not in attack_defense_mapping[at_type]:
                        continue  # 如果攻击类型和防御策略不匹配，则跳过

                    # 创建一个新的进程来运行实验
                    process = Process(target=train_attack_Defense_wrapper, args=(results, at_type, defense, n_attackers,seed_number))
                    process.start()  # 启动进程
                    process.join()  # 等待进程完成

        # 将结果转换为DataFrame并保存到CSV文件
        df = pd.DataFrame(list(results), columns=['Attack Type', 'Defense', 'Number of Attackers', 'Best Global Accuracy'])
        df.sort_values(by=['Attack Type', 'Defense', 'Number of Attackers'], inplace=True)
        df.reset_index(drop=True, inplace=True)

        df.to_csv('results.csv', index=False)
        print(df)
        print('Experiment Finished Successfully')


if __name__ == '__main__':
    run_experiments()
